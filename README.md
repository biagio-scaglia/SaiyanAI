# SaiyanAI ğŸ‰ - Dragon Ball Chatbot & Knowledge Base

Un assistente AI locale esperto su Dragon Ball, potenziato da **LangGraph**, **Qdrant** e **Llama-3.2**. Include un frontend moderno in Next.js con personalitÃ  multiple (Goku, Vegeta, Gohan, Frieza).

## ğŸš€ FunzionalitÃ 

- **RAG (Retrieval Augmented Generation)**: Risponde a domande basandosi su file markdown locali (Lore, Personaggi, Archi, Trasformazioni, Tecniche).
- **Ricerca Web**: Cerca su DuckDuckGo per notizie recenti o date di uscita.
- **PersonalitÃ  Multiple**:
  - **Goku**: Allegro, energico, sempre pronto a combattere.
  - **Vegeta**: Orgoglioso, arrogante, principe dei Saiyan.
  - **Gohan**: Intelligente, pacifico, studioso.
  - **Frieza**: Malvagio, calcolatore, imperatore dello spazio.
- **Privacy Locale**: Usa modelli GGUF e database vettoriale locale (Qdrant).
- **Interfaccia Moderna**: Design responsive con animazioni fluide e tema Dragon Ball.

---

## ğŸ› ï¸ Installazione e Setup

### Prerequisiti

- Python 3.10 o superiore
- Node.js 18+ e npm (per il frontend)
- Un modello GGUF nella cartella `models/` (Default: `models/Llama-3.2-3B-Instruct-Q4_K_M.gguf`)

### 1. Setup Backend (Python)

Installa le dipendenze Python:

```bash
pip install -r requirements.txt
```

### 2. Caricamento Conoscenza

Popola il database vettoriale con la conoscenza di Dragon Ball (file in `knowledge/`):

```bash
python scripts/load_knowledge.py
```

Questo script caricherÃ  tutti i file markdown dalla cartella `knowledge/` nel database vettoriale Qdrant.

### 3. Setup Frontend (Next.js)

Spostati nella cartella frontend e installa i pacchetti:

```bash
cd frontend
npm install
cd ..
```

---

## â–¶ï¸ Avvio del Progetto

Per far funzionare tutto, devi aprire **due terminali** separati.

### Terminale 1: Backend API

Avvia il server Python. Usa questo comando per evitare problemi di PATH:

```bash
python -m uvicorn app.main:app --reload
```

_Il server partirÃ  su `http://localhost:8000`._

### Terminale 2: Frontend Web

Avvia l'interfaccia grafica:

```bash
cd frontend
npm run dev
```

_L'app si aprirÃ  su `http://localhost:3000`._

---

## ğŸ“ Struttura del Progetto

```
dragon-ball/
â”œâ”€â”€ app/                    # Backend FastAPI + LangGraph
â”‚   â”œâ”€â”€ graph/              # Grafo LangGraph per il ragionamento
â”‚   â””â”€â”€ main.py             # Entry point FastAPI
â”œâ”€â”€ frontend/               # Frontend Next.js
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/            # Pagine Next.js
â”‚   â”‚   â”œâ”€â”€ components/    # Componenti React
â”‚   â”‚   â””â”€â”€ lib/            # Utilities
â”‚   â””â”€â”€ public/            # Immagini e asset statici
â”œâ”€â”€ knowledge/              # File Markdown con conoscenza Dragon Ball
â”‚   â”œâ”€â”€ dragonball.md
â”‚   â”œâ”€â”€ series_overview.md
â”‚   â”œâ”€â”€ transformations_extended.md
â”‚   â”œâ”€â”€ techniques.md
â”‚   â”œâ”€â”€ movies_specials.md
â”‚   â””â”€â”€ games.md
â”œâ”€â”€ models/                 # Modelli LLM locali (GGUF)
â”œâ”€â”€ qdrant_data/           # Database vettoriale Qdrant
â”œâ”€â”€ scripts/               # Script di utilitÃ 
â”‚   â”œâ”€â”€ load_knowledge.py  # Carica conoscenza in Qdrant
â”‚   â”œâ”€â”€ test_api.py        # Test API backend
â”‚   â””â”€â”€ test_chat.py       # Test chat
â””â”€â”€ requirements.txt       # Dipendenze Python
```

---

## ğŸ§ª Test e Verifica

### Script di Test Automatico

Puoi verificare se il backend risponde correttamente (Vector Store + Web Search) usando questo script:

```bash
python scripts/test_api.py
```

### Test Chat Interattivo

Per testare la chat direttamente dal terminale:

```bash
python scripts/test_chat.py
```

---

## ğŸ¯ Come Funziona

1. **Ricezione Domanda**: L'utente invia una domanda tramite l'interfaccia web.
2. **Ricerca Vettoriale**: Il sistema cerca nella knowledge base locale (Qdrant) per informazioni rilevanti.
3. **Ricerca Web (Opzionale)**: Se necessario, cerca informazioni aggiornate su DuckDuckGo.
4. **Generazione Risposta**: Llama-3.2 genera una risposta basata sul contesto trovato e sulla personalitÃ  selezionata.
5. **Visualizzazione**: La risposta viene mostrata all'utente con animazioni fluide.

---

## ğŸ”§ Configurazione

### Modello LLM

Il modello di default Ã¨ `Llama-3.2-3B-Instruct-Q4_K_M.gguf`. Puoi cambiarlo modificando il percorso in `app/main.py`.

### PersonalitÃ 

Le personalitÃ  sono definite in `frontend/src/components/CharacterGrid.tsx`. Puoi aggiungere nuovi personaggi modificando l'array `CHARACTERS`.

### Knowledge Base

Aggiungi nuovi file markdown nella cartella `knowledge/` e riesegui `load_knowledge.py` per aggiornare il database.

---

## ğŸ“ Tecnologie Utilizzate

- **Backend**: FastAPI, LangGraph, LangChain, Qdrant, Ollama
- **Frontend**: Next.js 15, React, TypeScript, Tailwind CSS, Framer Motion
- **AI**: Llama-3.2 (GGUF), Embeddings locali
- **Database**: Qdrant (vettoriale)

---

## ğŸ‘¨â€ğŸ’» Sviluppatore

**Developed by Biagio Scaglia**

---

## ğŸ“„ Licenza

Questo progetto Ã¨ stato creato a scopo educativo e dimostrativo.

---

## ğŸ› Troubleshooting

### Il backend non si avvia
- Verifica che Python 3.10+ sia installato
- Controlla che tutte le dipendenze siano installate: `pip install -r requirements.txt`
- Assicurati che il modello GGUF sia presente in `models/`

### Il frontend non si connette al backend
- Verifica che il backend sia in esecuzione su `http://localhost:8000`
- Controlla la console del browser per errori CORS

### La knowledge base Ã¨ vuota
- Esegui `python scripts/load_knowledge.py` per popolare il database
- Verifica che i file markdown siano presenti in `knowledge/`

---

## ğŸš€ Prossimi Sviluppi

- [ ] Supporto per piÃ¹ modelli LLM
- [ ] Aggiunta di piÃ¹ personaggi
- [ ] ModalitÃ  dark/light
- [ ] Export conversazioni
- [ ] Integrazione con API esterne per informazioni in tempo reale
